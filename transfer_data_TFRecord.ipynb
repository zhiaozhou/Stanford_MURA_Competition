{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T19:02:36.303556Z",
     "start_time": "2018-06-03T19:02:36.284856Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.# Copyr \n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "r\"\"\"Downloads and converts Flowers data to TFRecords of TF-Example protos.\n",
    "\n",
    "This module downloads the Flowers data, uncompresses it, reads the files\n",
    "that make up the Flowers data and creates two TFRecord datasets: one for train\n",
    "and one for test. Each TFRecord dataset is comprised of a set of TF-Example\n",
    "protocol buffers, each of which contain a single image and label.\n",
    "\n",
    "The script should take about a minute to run.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.insert(0,'/home/paperspace/Property_Value_CNN/slim')\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# The URL where the Flowers data can be downloaded.\n",
    "_DATA_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "\n",
    "# The number of images in the validation set.\n",
    "_NUM_VALIDATION = 2778\n",
    "\n",
    "# Seed for repeatability.\n",
    "_RANDOM_SEED = 0\n",
    "\n",
    "# The number of shards per dataset split.\n",
    "_NUM_SHARDS = 5\n",
    "\n",
    "\n",
    "class ImageReader(object):\n",
    "  \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    # Initializes function that decodes RGB JPEG data.\n",
    "    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "  def read_image_dims(self, sess, image_data):\n",
    "    image = self.decode_jpeg(sess, image_data)\n",
    "    return image.shape[0], image.shape[1]\n",
    "\n",
    "  def decode_jpeg(self, sess, image_data):\n",
    "    image = sess.run(self._decode_jpeg,\n",
    "                     feed_dict={self._decode_jpeg_data: image_data})\n",
    "    assert len(image.shape) == 3\n",
    "    assert image.shape[2] == 3\n",
    "    return image\n",
    "\n",
    "\n",
    "def _get_filenames_and_classes(dataset_dir):\n",
    "  \"\"\"Returns a list of filenames and inferred class names.\n",
    "\n",
    "  Args:\n",
    "    dataset_dir: A directory containing a set of subdirectories representing\n",
    "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "\n",
    "  Returns:\n",
    "    A list of image file paths, relative to `dataset_dir` and the list of\n",
    "    subdirectories, representing class names.\n",
    "  \"\"\"\n",
    "  flower_root = os.path.join(dataset_dir, 'data_all')\n",
    "  directories = []\n",
    "  class_names = []\n",
    "  for filename in os.listdir(flower_root):\n",
    "    path = os.path.join(flower_root, filename)\n",
    "    if os.path.isdir(path):\n",
    "      directories.append(path)\n",
    "      class_names.append(filename)\n",
    "\n",
    "  photo_filenames = []\n",
    "  for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "      path = os.path.join(directory, filename)\n",
    "      photo_filenames.append(path)\n",
    "\n",
    "  return photo_filenames, sorted(class_names)\n",
    "\n",
    "\n",
    "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "  output_filename = 'stanford_%s_%05d-of-%05d.tfrecord' % (\n",
    "      split_name, shard_id, _NUM_SHARDS)\n",
    "  return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "\n",
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "  \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "\n",
    "  Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "  \"\"\"\n",
    "  assert split_name in ['train', 'validation']\n",
    "\n",
    "  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    image_reader = ImageReader()\n",
    "\n",
    "    with tf.Session('') as sess:\n",
    "\n",
    "      for shard_id in range(_NUM_SHARDS):\n",
    "        output_filename = _get_dataset_filename(\n",
    "            dataset_dir, split_name, shard_id)\n",
    "\n",
    "        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "          start_ndx = shard_id * num_per_shard\n",
    "          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "          for i in range(start_ndx, end_ndx):\n",
    "            sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                i+1, len(filenames), shard_id))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Read the filename:\n",
    "            image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "            height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "            class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "            class_id = class_names_to_ids[class_name]\n",
    "\n",
    "            example = dataset_utils.image_to_tfexample(\n",
    "                image_data, b'jpg', height, width, class_id)\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "  sys.stdout.write('\\n')\n",
    "  sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _clean_up_temporary_files(dataset_dir):\n",
    "  \"\"\"Removes temporary files used to create the dataset.\n",
    "\n",
    "  Args:\n",
    "    dataset_dir: The directory where the temporary files are stored.\n",
    "  \"\"\"\n",
    "  filename = _DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dataset_dir, filename)\n",
    "  tf.gfile.Remove(filepath)\n",
    "\n",
    "  tmp_dir = os.path.join(dataset_dir, 'stanford_photos')\n",
    "  tf.gfile.DeleteRecursively(tmp_dir)\n",
    "\n",
    "\n",
    "def _dataset_exists(dataset_dir):\n",
    "  for split_name in ['train', 'validation']:\n",
    "    for shard_id in range(_NUM_SHARDS):\n",
    "      output_filename = _get_dataset_filename(\n",
    "          dataset_dir, split_name, shard_id)\n",
    "      if not tf.gfile.Exists(output_filename):\n",
    "        return False\n",
    "  return True\n",
    "\n",
    "\n",
    "def run(dataset_dir):\n",
    "  \"\"\"Runs the download and conversion operation.\n",
    "\n",
    "  Args:\n",
    "    dataset_dir: The dataset directory where the dataset is stored.\n",
    "  \"\"\"\n",
    "  if not tf.gfile.Exists(dataset_dir):\n",
    "    tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "  if _dataset_exists(dataset_dir):\n",
    "    print('Dataset files already exist. Exiting without re-creating them.')\n",
    "    return\n",
    "\n",
    "  # dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
    "  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n",
    "  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "\n",
    "  # Divide into train and test:\n",
    "  random.seed(_RANDOM_SEED)\n",
    "  random.shuffle(photo_filenames)\n",
    "  training_filenames = photo_filenames[_NUM_VALIDATION:]\n",
    "  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n",
    "\n",
    "  # First, convert the training and validation sets.\n",
    "    \n",
    "  _convert_dataset('train', training_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "  _convert_dataset('validation', validation_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "\n",
    "  # Finally, write the labels file:\n",
    "  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
    "\n",
    "  # _clean_up_temporary_files(dataset_dir)\n",
    "  print('\\nFinished converting the Flowers dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T19:13:29.439786Z",
     "start_time": "2018-06-03T19:10:13.791134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 34534/34534 shard 4\n",
      ">> Converting image 2778/2778 shard 4\n",
      "\n",
      "Finished converting the Flowers dataset!\n"
     ]
    }
   ],
   "source": [
    "run('/home/paperspace/stanford/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
